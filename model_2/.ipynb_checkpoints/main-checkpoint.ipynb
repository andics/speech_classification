{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the contributions made to the original code:\n",
    " * Implemented GPU parallelism. Added a _DistributedSampler_ as an alternative to _Sampler_.\n",
    " * Wrapped the model into a _DataParallel_.\n",
    " * Made modifications to the _collatebatch_ in order to pad vectors for multi-GPU tensor processing.\n",
    " * Managed to sucesfully train on 8 NVIDIA V40.\n",
    "\n",
    "Due to the simplicity of the model, the training on multiple GPUs actually caused slower progress. The *second portion* of this notebook proceeds to train on a single GPU only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1gYr_aoNDue"
   },
   "source": [
    "# Multi-GPU training: setup of Data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "cd7hoGhYtbXQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    " \n",
    "class myDataset(Dataset):\n",
    "  def __init__(self, data_dir, segment_len=128):\n",
    "    self.data_dir = data_dir\n",
    "    self.segment_len = segment_len\n",
    " \n",
    "    # Load the mapping from speaker neme to their corresponding id. \n",
    "    mapping_path = Path(data_dir) / \"mapping.json\"\n",
    "    mapping = json.load(mapping_path.open())\n",
    "    self.speaker2id = mapping[\"speaker2id\"]\n",
    " \n",
    "    # Load metadata of training data.\n",
    "    metadata_path = Path(data_dir) / \"metadata.json\"\n",
    "    metadata = json.load(open(metadata_path))[\"speakers\"]\n",
    " \n",
    "    # Get the total number of speaker.\n",
    "    self.speaker_num = len(metadata.keys())\n",
    "    self.data = []\n",
    "    for speaker in metadata.keys():\n",
    "      for utterances in metadata[speaker]:\n",
    "        self.data.append([utterances[\"feature_path\"], self.speaker2id[speaker]])\n",
    " \n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    " \n",
    "  def __getitem__(self, index):\n",
    "    feat_path, speaker = self.data[index]\n",
    "    # Load preprocessed mel-spectrogram.\n",
    "    mel = torch.load(os.path.join(self.data_dir, feat_path))\n",
    " \n",
    "    # Segmemt mel-spectrogram into \"segment_len\" frames.\n",
    "    if len(mel) > self.segment_len:\n",
    "      # Randomly get the starting point of the segment.\n",
    "      start = random.randint(0, len(mel) - self.segment_len)\n",
    "      # Get a segment with \"segment_len\" frames.\n",
    "      mel = torch.FloatTensor(mel[start:start+self.segment_len])\n",
    "    else:\n",
    "      mel = torch.FloatTensor(mel)\n",
    "    # Turn the speaker id into long for computing loss later.\n",
    "    speaker = torch.FloatTensor([speaker]).long()\n",
    "    return mel, speaker\n",
    " \n",
    "  def get_speaker_number(self):\n",
    "    return self.speaker_num\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "  # Process features within a batch.\n",
    "  \"\"\"Collate a batch of data.\"\"\"\n",
    "  mel, speaker = zip(*batch)\n",
    "  # Because we train the model batch by batch, we need to pad the features in the same batch to make their lengths the same.\n",
    "  mel = pad_sequence(mel, batch_first=True, padding_value=-20)    # pad log 10^(-20) which is very small value.\n",
    "  # mel: (batch size, length, 40)\n",
    "  return mel, torch.FloatTensor(speaker).long()\n",
    "\n",
    "\n",
    "def get_dataloader(data_dir, batch_size, n_workers):\n",
    "    dataset = myDataset(data_dir)\n",
    "    speaker_num = dataset.get_speaker_number()\n",
    "    trainlen = int(0.9 * len(dataset))\n",
    "    lengths = [trainlen, len(dataset) - trainlen]\n",
    "    trainset, validset = random_split(dataset, lengths)\n",
    "\n",
    "    train_sampler = DistributedSampler(trainset)\n",
    "    valid_sampler = DistributedSampler(validset)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        trainset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=train_sampler,\n",
    "        num_workers=n_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_batch,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        validset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=valid_sampler,\n",
    "        num_workers=n_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_batch,\n",
    "    )\n",
    "    return train_loader, valid_loader, speaker_num\n",
    "\n",
    "\n",
    "class ConformerBlock(nn.Module):\n",
    "  def __init__(self, d_model, nhead, dim_feedforward=512, dropout=0.1, activation=\"relu\"):\n",
    "    super(ConformerBlock, self).__init__()\n",
    "    self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "    self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "    self.norm1 = nn.LayerNorm(d_model)\n",
    "    self.norm2 = nn.LayerNorm(d_model)\n",
    "    self.dropout1 = nn.Dropout(dropout)\n",
    "    self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    self.activation = getattr(F, activation)\n",
    "\n",
    "    self.conv = nn.Sequential(\n",
    "        nn.LayerNorm(d_model),\n",
    "        nn.Conv1d(d_model, 2 * d_model, 1),\n",
    "        nn.GLU(),\n",
    "        nn.DepthwiseConv1d(d_model, d_model, 3, padding=1),\n",
    "        nn.BatchNorm1d(d_model),\n",
    "        nn.Conv1d(d_model, d_model, 1)\n",
    "    )\n",
    "\n",
    "  def forward(self, src):\n",
    "    src2 = self.norm1(src)\n",
    "    src = src + self.dropout1(self.self_attn(src2, src2, src2)[0])\n",
    "    src2 = self.norm2(src)\n",
    "    src = src + self.dropout2(self.linear2(self.activation(self.linear1(src2))))\n",
    "    return src\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "  def __init__(self, d_model=80, n_spks=600, dropout=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.prenet = nn.Linear(40, d_model)\n",
    "    self.conformer = ConformerBlock(d_model, nhead=2, dim_feedforward=128)\n",
    "\n",
    "    self.pred_layer = nn.Sequential(\n",
    "      nn.Linear(d_model, d_model),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(d_model, n_spks)\n",
    "    )\n",
    "\n",
    "  def forward(self, mels):\n",
    "    out = self.prenet(mels)\n",
    "    out = out.permute(1, 0, 2)\n",
    "    out = self.conformer(out)\n",
    "    out = out.transpose(0, 1)\n",
    "    stats = out.mean(dim=1)\n",
    "\n",
    "    out = self.pred_layer(stats)\n",
    "    return out\n",
    "  \n",
    "\n",
    "def get_cosine_schedule_with_warmup(\n",
    "  optimizer: Optimizer,\n",
    "  num_warmup_steps: int,\n",
    "  num_training_steps: int,\n",
    "  num_cycles: float = 0.5,\n",
    "  last_epoch: int = -1,\n",
    "):\n",
    "  def lr_lambda(current_step):\n",
    "    # Warmup\n",
    "    if current_step < num_warmup_steps:\n",
    "      return float(current_step) / float(max(1, num_warmup_steps))\n",
    "    # decadence\n",
    "    progress = float(current_step - num_warmup_steps) / float(\n",
    "      max(1, num_training_steps - num_warmup_steps)\n",
    "    )\n",
    "    return max(\n",
    "      0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))\n",
    "    )\n",
    "\n",
    "  return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
    "\n",
    "\n",
    "def model_fn(batch, model, criterion, device):\n",
    "    \"\"\"Forward a batch through the model.\"\"\"\n",
    "\n",
    "    mels, labels = batch\n",
    "    mels = mels.to(device)\n",
    "    labels = labels.to(device)\n",
    "    labels = labels.flatten()  # Flatten labels to 1D\n",
    "\n",
    "    outs = model(mels)\n",
    "\n",
    "    loss = criterion(outs, labels)\n",
    "\n",
    "    # Get the speaker id with highest probability.\n",
    "    preds = outs.argmax(1)\n",
    "    # Compute accuracy.\n",
    "    accuracy = torch.mean((preds == labels).float())\n",
    "\n",
    "    return loss, accuracy\n",
    "\n",
    "def valid(dataloader, model, criterion, device): \n",
    "  \"\"\"Validate on validation set.\"\"\"\n",
    "\n",
    "  model.eval()\n",
    "  running_loss = 0.0\n",
    "  running_accuracy = 0.0\n",
    "  pbar = tqdm(total=len(dataloader.dataset), ncols=0, desc=\"Valid\", unit=\" uttr\")\n",
    "\n",
    "  for i, batch in enumerate(dataloader):\n",
    "    with torch.no_grad():\n",
    "      loss, accuracy = model_fn(batch, model, criterion, device)\n",
    "      running_loss += loss.item()\n",
    "      running_accuracy += accuracy.item()\n",
    "\n",
    "    pbar.update(dataloader.batch_size)\n",
    "    pbar.set_postfix(\n",
    "      loss=f\"{running_loss / (i+1):.2f}\",\n",
    "      accuracy=f\"{running_accuracy / (i+1):.2f}\",\n",
    "    )\n",
    "\n",
    "  pbar.close()\n",
    "  model.train()\n",
    "\n",
    "  return running_accuracy / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noHXyal5p1W5"
   },
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "chRQE7oYtw62",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   2% 31/2000 [00:14<15:51,  2.07 step/s, accuracy=0.24, loss=6.16, step=31]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: epoch is taking too long compared to single GPU (7888 > 120 sec).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"Arguments\"\"\"\n",
    "    config = {\n",
    "        \"data_dir\": \"../Dataset\",\n",
    "        \"save_path\": \"model.ckpt\",\n",
    "        \"batch_size\": 256,\n",
    "        \"n_workers\": 0,\n",
    "        \"valid_steps\": 2000,\n",
    "        \"warmup_steps\": 1000,\n",
    "        \"save_steps\": 4000,\n",
    "        \"total_steps\": 70000,\n",
    "    }\n",
    "    return config\n",
    "\n",
    "\n",
    "def main_multi_gpu(data_dir, save_path, batch_size, n_workers, valid_steps, warmup_steps, total_steps, save_steps):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    dataset = myDataset(data_dir)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, num_workers=n_workers)\n",
    "\n",
    "    model = Classifier(d_model=80, n_spks=600, dropout=0.1).to(device)\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using\", torch.cuda.device_count(), \"GPUs\")\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-3)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "    best_accuracy = -1.0\n",
    "    best_state_dict = None\n",
    "\n",
    "    pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n",
    "    time_limit = 120  # 2 minute in seconds\n",
    "\n",
    "    for step in range(total_steps):\n",
    "        batch = next(iter(train_loader))\n",
    "        loss, accuracy = model_fn(batch, model, criterion, device)\n",
    "\n",
    "        if step == 0:\n",
    "            start_time = time.time()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        estimated_time_left = (total_steps - step) * (elapsed_time / (step+1))\n",
    "        if step>30 and estimated_time_left > time_limit:\n",
    "            print(f'Stopping training: epoch is taking too long compared to single GPU ({int(estimated_time_left)} > {time_limit} sec).')\n",
    "            return\n",
    "\n",
    "        pbar.update()\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.2f}\", accuracy=f\"{accuracy.item():.2f}\", step=step + 1)\n",
    "\n",
    "        if (step + 1) % valid_steps == 0:\n",
    "            pbar.close()\n",
    "            valid_accuracy = valid(train_loader, model, criterion, device)\n",
    "\n",
    "            if valid_accuracy > best_accuracy:\n",
    "                best_accuracy = valid_accuracy\n",
    "                best_state_dict = model.state_dict()\n",
    "                pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n",
    "\n",
    "        if (step + 1) % save_steps == 0 and best_state_dict:\n",
    "            torch.save(best_state_dict, save_path)\n",
    "            pbar.write(f\"Step {step + 1}, best model saved. (accuracy={best_accuracy:.4f})\")\n",
    "    pbar.close()\n",
    "\n",
    "main_multi_gpu(**parse_args())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0R2rx3AyHpQ-"
   },
   "source": [
    "# Single GPU training\n",
    "\n",
    "As no significant speedup was observed, we proceed with single GPU trianing. We redefine some important functions for single GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(data_dir, batch_size, n_workers):\n",
    "  \"\"\"Generate dataloader\"\"\"\n",
    "  dataset = myDataset(data_dir)\n",
    "  speaker_num = dataset.get_speaker_number()\n",
    "  # Split dataset into training dataset and validation dataset\n",
    "  trainlen = int(0.9 * len(dataset))\n",
    "  lengths = [trainlen, len(dataset) - trainlen]\n",
    "  trainset, validset = random_split(dataset, lengths)\n",
    "  train_loader = DataLoader(\n",
    "    trainset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=n_workers,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_batch,\n",
    "  )\n",
    "  valid_loader = DataLoader(\n",
    "    validset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=n_workers,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_batch,\n",
    "  )\n",
    "  return train_loader, valid_loader, speaker_num\n",
    "\n",
    "def model_fn(batch, model, criterion, device):\n",
    "  \"\"\"Forward a batch through the model.\"\"\"\n",
    "\n",
    "  mels, labels = batch\n",
    "  mels = mels.to(device)\n",
    "  labels = labels.to(device)\n",
    "\n",
    "  outs = model(mels)\n",
    "\n",
    "  loss = criterion(outs, labels)\n",
    "\n",
    "  # Get the speaker id with highest probability.\n",
    "  preds = outs.argmax(1)\n",
    "  # Compute accuracy.\n",
    "  accuracy = torch.mean((preds == labels).float())\n",
    "\n",
    "  return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main single GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0% 0/2000 [02:33<?, ? step/s]\n",
      "Train:   0% 0/2000 [02:22<?, ? step/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info]: Use cuda now!\n",
      "[Info]: Finish loading data!\n",
      "[Info]: Finish creating model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train:  13% 253/2000 [00:08<00:54, 32.24 step/s, accuracy=0.00, loss=6.35, step=253]\n",
      "Train: 100% 2000/2000 [01:03<00:00, 31.41 step/s, accuracy=0.03, loss=5.27, step=2000]A\n",
      "Valid: 100% 4832/4836 [00:03<00:00, 1333.35 uttr/s, accuracy=0.07, loss=5.01]\n",
      "Train: 100% 2000/2000 [01:05<00:00, 30.76 step/s, accuracy=0.06, loss=4.80, step=4000]\n",
      "Valid: 100% 4832/4836 [00:03<00:00, 1358.16 uttr/s, accuracy=0.12, loss=4.35]\n",
      "Train:   0% 0/2000 [00:00<?, ? step/s]\n",
      "                                                                                    \n",
      "Train:   0% 6/2000 [00:00<01:21, 24.53 step/s, accuracy=0.09, loss=4.62, step=4006]]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4000, best model saved. (accuracy=0.1229)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100% 2000/2000 [01:06<00:00, 29.87 step/s, accuracy=0.03, loss=4.53, step=6000]\n",
      "Valid: 100% 4832/4836 [00:03<00:00, 1329.89 uttr/s, accuracy=0.17, loss=4.00]\n",
      "Train: 100% 2000/2000 [01:06<00:00, 29.90 step/s, accuracy=0.12, loss=4.58, step=8000]\n",
      "Valid: 100% 4832/4836 [00:03<00:00, 1294.42 uttr/s, accuracy=0.17, loss=4.00]\n",
      "Train:   0% 0/2000 [00:00<?, ? step/s]\n",
      "                                                                                    \n",
      "Train:   0% 5/2000 [00:00<01:43, 19.24 step/s, accuracy=0.16, loss=4.41, step=8005]]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8000, best model saved. (accuracy=0.1740)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100% 2000/2000 [01:06<00:00, 29.86 step/s, accuracy=0.22, loss=3.81, step=1e+4]\n",
      "Valid:  44% 2112/4836 [00:01<00:02, 1316.65 uttr/s, accuracy=0.21, loss=3.76]"
     ]
    }
   ],
   "source": [
    "def parse_args():\n",
    "  \"\"\"arguments\"\"\"\n",
    "  config = {\n",
    "    \"data_dir\": \"../Dataset\",\n",
    "    \"save_path\": \"model.ckpt\",\n",
    "    \"batch_size\": 32,\n",
    "    \"n_workers\": 0,\n",
    "    \"valid_steps\": 2000,\n",
    "    \"warmup_steps\": 1000,\n",
    "    \"save_steps\": 4000,\n",
    "    \"total_steps\": 70000,\n",
    "  }\n",
    "\n",
    "  return config\n",
    "\n",
    "\n",
    "def main(\n",
    "  data_dir,\n",
    "  save_path,\n",
    "  batch_size,\n",
    "  n_workers,\n",
    "  valid_steps,\n",
    "  warmup_steps,\n",
    "  total_steps,\n",
    "  save_steps,\n",
    "):\n",
    "  \"\"\"Main function.\"\"\"\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  print(f\"[Info]: Use {device} now!\")\n",
    "\n",
    "  train_loader, valid_loader, speaker_num = get_dataloader(data_dir, batch_size, n_workers)\n",
    "  train_iterator = iter(train_loader)\n",
    "  print(f\"[Info]: Finish loading data!\", flush = True)\n",
    "\n",
    "  model = Classifier(n_spks=speaker_num).to(device)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = AdamW(model.parameters(), lr=1e-3)\n",
    "  scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "  print(f\"[Info]: Finish creating model!\", flush = True)\n",
    "\n",
    "  best_accuracy = -1.0\n",
    "  best_state_dict = None\n",
    "\n",
    "  pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n",
    "\n",
    "  for step in range(total_steps):\n",
    "    # Get data\n",
    "    try:\n",
    "      batch = next(train_iterator)\n",
    "    except StopIteration:\n",
    "      train_iterator = iter(train_loader)\n",
    "      batch = next(train_iterator)\n",
    "\n",
    "    loss, accuracy = model_fn(batch, model, criterion, device)\n",
    "    batch_loss = loss.item()\n",
    "    batch_accuracy = accuracy.item()\n",
    "\n",
    "    # Updata model\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Log\n",
    "    pbar.update()\n",
    "    pbar.set_postfix(\n",
    "      loss=f\"{batch_loss:.2f}\",\n",
    "      accuracy=f\"{batch_accuracy:.2f}\",\n",
    "      step=step + 1,\n",
    "    )\n",
    "\n",
    "    # Do validation\n",
    "    if (step + 1) % valid_steps == 0:\n",
    "      pbar.close()\n",
    "\n",
    "      valid_accuracy = valid(valid_loader, model, criterion, device)\n",
    "\n",
    "      # keep the best model\n",
    "      if valid_accuracy > best_accuracy:\n",
    "        best_accuracy = valid_accuracy\n",
    "        best_state_dict = model.state_dict()\n",
    "\n",
    "      pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n",
    "\n",
    "    # Save the best model so far.\n",
    "    if (step + 1) % save_steps == 0 and best_state_dict is not None:\n",
    "      save_path = f\"model_{step + 1}_{best_accuracy:.4f}.ckpt\"\n",
    "      torch.save(best_state_dict, save_path)\n",
    "      pbar.write(f\"Step {step + 1}, best model saved. (accuracy={best_accuracy:.4f})\")\n",
    "\n",
    "  pbar.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main(**parse_args())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSuI3WY9Fz78"
   },
   "source": [
    "## Dataset of inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4evns0055Dsx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "  def __init__(self, data_dir):\n",
    "    testdata_path = Path(data_dir) / \"testdata.json\"\n",
    "    metadata = json.load(testdata_path.open())\n",
    "    self.data_dir = data_dir\n",
    "    self.data = metadata[\"utterances\"]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    utterance = self.data[index]\n",
    "    feat_path = utterance[\"feature_path\"]\n",
    "    mel = torch.load(os.path.join(self.data_dir, feat_path))\n",
    "\n",
    "    return feat_path, mel\n",
    "\n",
    "\n",
    "def inference_collate_batch(batch):\n",
    "  \"\"\"Collate a batch of data.\"\"\"\n",
    "  feat_paths, mels = zip(*batch)\n",
    "\n",
    "  return feat_paths, torch.stack(mels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oAinHBG1GIWv"
   },
   "source": [
    "## Main funcrion of Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "yQaTt7VDHoRI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info]: Use cuda now!\n",
      "[Info]: Finish loading data!\n",
      "[Info]: Finish creating model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                               | 0/6657 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|▋                                                                                    | 52/6657 [00:00<00:12, 518.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|█▎                                                                                  | 104/6657 [00:00<00:13, 499.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|█▉                                                                                  | 154/6657 [00:00<00:13, 497.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|██▌                                                                                 | 206/6657 [00:00<00:12, 504.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|███▏                                                                                | 257/6657 [00:00<00:12, 496.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|███▉                                                                                | 309/6657 [00:00<00:12, 503.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|████▌                                                                               | 360/6657 [00:00<00:12, 502.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|█████▏                                                                              | 413/6657 [00:00<00:12, 507.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|█████▊                                                                              | 465/6657 [00:00<00:12, 511.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|██████▌                                                                             | 518/6657 [00:01<00:11, 515.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|███████▏                                                                            | 572/6657 [00:01<00:11, 520.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|███████▉                                                                            | 626/6657 [00:01<00:11, 524.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|████████▌                                                                           | 680/6657 [00:01<00:11, 526.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█████████▏                                                                          | 733/6657 [00:01<00:11, 515.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█████████▉                                                                          | 785/6657 [00:01<00:11, 513.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|██████████▌                                                                         | 837/6657 [00:01<00:11, 512.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|███████████▏                                                                        | 889/6657 [00:01<00:11, 513.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|███████████▊                                                                        | 941/6657 [00:01<00:11, 514.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|████████████▌                                                                       | 993/6657 [00:01<00:11, 509.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█████████████                                                                      | 1044/6657 [00:02<00:11, 507.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█████████████▋                                                                     | 1096/6657 [00:02<00:10, 508.94it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|██████████████▎                                                                    | 1150/6657 [00:02<00:10, 515.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|██████████████▉                                                                    | 1203/6657 [00:02<00:10, 520.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|███████████████▋                                                                   | 1256/6657 [00:02<00:11, 486.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|████████████████▎                                                                  | 1306/6657 [00:02<00:11, 463.48it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|████████████████▉                                                                  | 1356/6657 [00:02<00:11, 472.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|█████████████████▌                                                                 | 1409/6657 [00:02<00:10, 486.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██████████████████▏                                                                | 1461/6657 [00:02<00:10, 495.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██████████████████▉                                                                | 1515/6657 [00:02<00:10, 505.71it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|███████████████████▌                                                               | 1566/6657 [00:03<00:10, 506.30it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|████████████████████▏                                                              | 1618/6657 [00:03<00:09, 509.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|████████████████████▊                                                              | 1670/6657 [00:03<00:09, 506.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|█████████████████████▍                                                             | 1721/6657 [00:03<00:09, 506.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██████████████████████                                                             | 1773/6657 [00:03<00:09, 508.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██████████████████████▊                                                            | 1825/6657 [00:03<00:09, 509.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|███████████████████████▍                                                           | 1878/6657 [00:03<00:09, 513.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|████████████████████████                                                           | 1931/6657 [00:03<00:09, 517.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|████████████████████████▋                                                          | 1985/6657 [00:03<00:08, 523.75it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|█████████████████████████▍                                                         | 2038/6657 [00:04<00:08, 523.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|██████████████████████████                                                         | 2092/6657 [00:04<00:08, 525.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|██████████████████████████▊                                                        | 2146/6657 [00:04<00:08, 529.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███████████████████████████▍                                                       | 2199/6657 [00:04<00:08, 524.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|████████████████████████████                                                       | 2252/6657 [00:04<00:08, 523.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|████████████████████████████▋                                                      | 2305/6657 [00:04<00:08, 522.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|█████████████████████████████▍                                                     | 2358/6657 [00:04<00:08, 518.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|██████████████████████████████                                                     | 2411/6657 [00:04<00:08, 521.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|██████████████████████████████▋                                                    | 2465/6657 [00:04<00:07, 524.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███████████████████████████████▍                                                   | 2518/6657 [00:04<00:07, 521.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|████████████████████████████████                                                   | 2572/6657 [00:05<00:07, 526.92it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|████████████████████████████████▋                                                  | 2626/6657 [00:05<00:07, 529.72it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|█████████████████████████████████▍                                                 | 2681/6657 [00:05<00:07, 532.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|██████████████████████████████████                                                 | 2735/6657 [00:05<00:07, 523.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|██████████████████████████████████▊                                                | 2788/6657 [00:05<00:07, 521.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|███████████████████████████████████▍                                               | 2841/6657 [00:05<00:07, 521.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████████████████████████████████████                                               | 2895/6657 [00:05<00:07, 524.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████████████████████████████████████▊                                              | 2948/6657 [00:05<00:07, 525.94it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|█████████████████████████████████████▍                                             | 3001/6657 [00:05<00:06, 524.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|██████████████████████████████████████                                             | 3054/6657 [00:05<00:06, 519.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|██████████████████████████████████████▋                                            | 3106/6657 [00:06<00:06, 513.99it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|███████████████████████████████████████▎                                           | 3158/6657 [00:06<00:06, 508.93it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████████████████████████████████████████                                           | 3209/6657 [00:06<00:06, 506.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████████████████████████████████████████▋                                          | 3261/6657 [00:06<00:06, 507.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████████████████████████████████████████▎                                         | 3312/6657 [00:06<00:06, 502.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████████████████████████████████████████▉                                         | 3363/6657 [00:06<00:06, 497.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|██████████████████████████████████████████▌                                        | 3415/6657 [00:06<00:06, 501.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|███████████████████████████████████████████▏                                       | 3466/6657 [00:06<00:06, 501.69it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|███████████████████████████████████████████▉                                       | 3519/6657 [00:06<00:06, 509.73it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|████████████████████████████████████████████▌                                      | 3573/6657 [00:06<00:05, 517.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████████████████████████████████████████████▏                                     | 3627/6657 [00:07<00:05, 523.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████████████████████████████████████████████▉                                     | 3682/6657 [00:07<00:05, 528.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|██████████████████████████████████████████████▌                                    | 3735/6657 [00:07<00:05, 528.68it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|███████████████████████████████████████████████▏                                   | 3788/6657 [00:07<00:05, 522.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|███████████████████████████████████████████████▉                                   | 3841/6657 [00:07<00:05, 518.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|████████████████████████████████████████████████▌                                  | 3894/6657 [00:07<00:05, 520.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████████████████████████████████████████████████▏                                 | 3947/6657 [00:07<00:05, 517.75it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|█████████████████████████████████████████████████▊                                 | 4000/6657 [00:07<00:05, 517.89it/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████████████████████████████████████████████████▌                                | 4052/6657 [00:07<00:05, 514.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|███████████████████████████████████████████████████▏                               | 4104/6657 [00:08<00:06, 418.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|███████████████████████████████████████████████████▋                               | 4149/6657 [00:08<00:07, 326.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|████████████████████████████████████████████████████▏                              | 4187/6657 [00:08<00:08, 285.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|████████████████████████████████████████████████████▌                              | 4220/6657 [00:08<00:09, 261.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|████████████████████████████████████████████████████▉                              | 4249/6657 [00:08<00:09, 251.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|█████████████████████████████████████████████████████▎                             | 4276/6657 [00:08<00:09, 246.62it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|█████████████████████████████████████████████████████▋                             | 4302/6657 [00:09<00:09, 240.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|█████████████████████████████████████████████████████▉                             | 4327/6657 [00:09<00:09, 234.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████████████████████████████████████████████████████▏                            | 4351/6657 [00:09<00:10, 227.72it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████████████████████████████████████████████████████▌                            | 4374/6657 [00:09<00:10, 221.16it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████████████████████████████████████████████████████▊                            | 4397/6657 [00:09<00:10, 217.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|███████████████████████████████████████████████████████                            | 4419/6657 [00:09<00:10, 214.88it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|███████████████████████████████████████████████████████▎                           | 4441/6657 [00:09<00:10, 211.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|███████████████████████████████████████████████████████▋                           | 4463/6657 [00:09<00:10, 211.72it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|███████████████████████████████████████████████████████▉                           | 4486/6657 [00:09<00:10, 214.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|████████████████████████████████████████████████████████▏                          | 4508/6657 [00:09<00:10, 212.68it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|████████████████████████████████████████████████████████▍                          | 4530/6657 [00:10<00:09, 213.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|████████████████████████████████████████████████████████▊                          | 4552/6657 [00:10<00:09, 214.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|█████████████████████████████████████████████████████████                          | 4574/6657 [00:10<00:09, 208.91it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|█████████████████████████████████████████████████████████▎                         | 4596/6657 [00:10<00:09, 209.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|█████████████████████████████████████████████████████████▌                         | 4619/6657 [00:10<00:09, 211.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|█████████████████████████████████████████████████████████▊                         | 4641/6657 [00:10<00:09, 212.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|██████████████████████████████████████████████████████████▏                        | 4663/6657 [00:10<00:09, 211.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|██████████████████████████████████████████████████████████▍                        | 4685/6657 [00:10<00:09, 212.97it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|██████████████████████████████████████████████████████████▋                        | 4707/6657 [00:10<00:09, 213.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|██████████████████████████████████████████████████████████▉                        | 4729/6657 [00:11<00:09, 212.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████████████████████████████████████████████████████████▏                       | 4751/6657 [00:11<00:08, 212.93it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████████████████████████████████████████████████████████▌                       | 4773/6657 [00:11<00:09, 208.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████████████████████████████████████████████████████████▊                       | 4794/6657 [00:11<00:08, 207.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|████████████████████████████████████████████████████████████                       | 4815/6657 [00:11<00:08, 207.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|████████████████████████████████████████████████████████████▎                      | 4836/6657 [00:11<00:08, 204.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|████████████████████████████████████████████████████████████▌                      | 4860/6657 [00:11<00:08, 214.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|████████████████████████████████████████████████████████████▊                      | 4882/6657 [00:11<00:08, 213.99it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|█████████████████████████████████████████████████████████████▏                     | 4905/6657 [00:11<00:08, 218.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|█████████████████████████████████████████████████████████████▍                     | 4927/6657 [00:11<00:07, 217.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|█████████████████████████████████████████████████████████████▋                     | 4949/6657 [00:12<00:07, 214.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|█████████████████████████████████████████████████████████████▉                     | 4971/6657 [00:12<00:07, 211.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|██████████████████████████████████████████████████████████████▎                    | 4993/6657 [00:12<00:07, 213.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|██████████████████████████████████████████████████████████████▌                    | 5015/6657 [00:12<00:07, 215.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|██████████████████████████████████████████████████████████████▊                    | 5037/6657 [00:12<00:07, 210.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████████████████████████████████████████████████████████████                    | 5059/6657 [00:12<00:07, 211.94it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████████████████████████████████████████████████████████████▎                   | 5081/6657 [00:12<00:07, 212.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████████████████████████████████████████████████████████████▌                   | 5103/6657 [00:12<00:07, 205.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████████████████████████████████████████████████████████████▉                   | 5126/6657 [00:12<00:07, 210.13it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|████████████████████████████████████████████████████████████████▏                  | 5149/6657 [00:13<00:07, 214.68it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|████████████████████████████████████████████████████████████████▍                  | 5171/6657 [00:13<00:06, 214.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|████████████████████████████████████████████████████████████████▋                  | 5193/6657 [00:13<00:06, 212.81it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|█████████████████████████████████████████████████████████████████                  | 5215/6657 [00:13<00:06, 210.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|█████████████████████████████████████████████████████████████████▎                 | 5237/6657 [00:13<00:06, 212.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|█████████████████████████████████████████████████████████████████▌                 | 5260/6657 [00:13<00:06, 216.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|█████████████████████████████████████████████████████████████████▊                 | 5282/6657 [00:13<00:06, 209.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|██████████████████████████████████████████████████████████████████▏                | 5304/6657 [00:13<00:06, 211.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|██████████████████████████████████████████████████████████████████▍                | 5327/6657 [00:13<00:06, 214.57it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|██████████████████████████████████████████████████████████████████▋                | 5349/6657 [00:13<00:06, 209.57it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|██████████████████████████████████████████████████████████████████▉                | 5372/6657 [00:14<00:06, 213.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|███████████████████████████████████████████████████████████████████▎               | 5395/6657 [00:14<00:05, 215.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|███████████████████████████████████████████████████████████████████▌               | 5418/6657 [00:14<00:05, 217.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|███████████████████████████████████████████████████████████████████▊               | 5440/6657 [00:14<00:05, 214.98it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████████████████████████████████████████████████████████████████               | 5462/6657 [00:14<00:05, 208.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████████████████████████████████████████████████████████████████▎              | 5483/6657 [00:14<00:05, 203.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████████████████████████████████████████████████████████████████▌              | 5504/6657 [00:14<00:05, 202.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████████████████████████████████████████████████████████████████▉              | 5525/6657 [00:14<00:05, 204.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|█████████████████████████████████████████████████████████████████████▏             | 5546/6657 [00:14<00:05, 204.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|█████████████████████████████████████████████████████████████████████▍             | 5569/6657 [00:15<00:05, 211.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|█████████████████████████████████████████████████████████████████████▋             | 5591/6657 [00:15<00:05, 208.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|█████████████████████████████████████████████████████████████████████▉             | 5612/6657 [00:15<00:05, 205.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|██████████████████████████████████████████████████████████████████████▏            | 5633/6657 [00:15<00:05, 202.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|██████████████████████████████████████████████████████████████████████▍            | 5654/6657 [00:15<00:04, 204.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|██████████████████████████████████████████████████████████████████████▊            | 5675/6657 [00:15<00:04, 203.91it/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|███████████████████████████████████████████████████████████████████████            | 5696/6657 [00:15<00:04, 204.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|███████████████████████████████████████████████████████████████████████▎           | 5717/6657 [00:15<00:04, 204.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|███████████████████████████████████████████████████████████████████████▌           | 5738/6657 [00:15<00:04, 202.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|███████████████████████████████████████████████████████████████████████▊           | 5761/6657 [00:15<00:04, 207.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████████████████████████████████████████████████████████████████████           | 5783/6657 [00:16<00:04, 209.76it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████████████████████████████████████████████████████████████████████▎          | 5804/6657 [00:16<00:04, 209.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████████████████████████████████████████████████████████████████████▋          | 5825/6657 [00:16<00:04, 207.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████████████████████████████████████████████████████████████████████▉          | 5847/6657 [00:16<00:03, 210.93it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|█████████████████████████████████████████████████████████████████████████▏         | 5869/6657 [00:16<00:03, 208.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|█████████████████████████████████████████████████████████████████████████▍         | 5890/6657 [00:16<00:03, 206.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|█████████████████████████████████████████████████████████████████████████▋         | 5913/6657 [00:16<00:03, 211.57it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|█████████████████████████████████████████████████████████████████████████▉         | 5935/6657 [00:16<00:03, 208.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|██████████████████████████████████████████████████████████████████████████▎        | 5956/6657 [00:16<00:03, 207.79it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|██████████████████████████████████████████████████████████████████████████▌        | 5977/6657 [00:16<00:03, 205.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|██████████████████████████████████████████████████████████████████████████▊        | 5998/6657 [00:17<00:03, 206.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|███████████████████████████████████████████████████████████████████████████        | 6019/6657 [00:17<00:03, 204.77it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|███████████████████████████████████████████████████████████████████████████▎       | 6041/6657 [00:17<00:02, 209.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|███████████████████████████████████████████████████████████████████████████▌       | 6062/6657 [00:17<00:02, 205.77it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|███████████████████████████████████████████████████████████████████████████▊       | 6084/6657 [00:17<00:02, 208.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|████████████████████████████████████████████████████████████████████████████       | 6105/6657 [00:17<00:02, 208.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|████████████████████████████████████████████████████████████████████████████▍      | 6126/6657 [00:17<00:02, 205.76it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|████████████████████████████████████████████████████████████████████████████▋      | 6147/6657 [00:17<00:02, 204.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|████████████████████████████████████████████████████████████████████████████▉      | 6168/6657 [00:17<00:02, 205.99it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████████████████████████████████████████████████████████████████████████▏     | 6189/6657 [00:18<00:02, 205.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████████████████████████████████████████████████████████████████████████▍     | 6211/6657 [00:18<00:02, 207.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████████████████████████████████████████████████████████████████████████▋     | 6232/6657 [00:18<00:02, 206.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████████████████████████████████████████████████████████████████████████▉     | 6253/6657 [00:18<00:01, 204.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|██████████████████████████████████████████████████████████████████████████████▏    | 6274/6657 [00:18<00:01, 203.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|██████████████████████████████████████████████████████████████████████████████▌    | 6297/6657 [00:18<00:01, 208.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|██████████████████████████████████████████████████████████████████████████████▊    | 6318/6657 [00:18<00:01, 205.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|███████████████████████████████████████████████████████████████████████████████    | 6340/6657 [00:18<00:01, 206.88it/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|███████████████████████████████████████████████████████████████████████████████▎   | 6361/6657 [00:18<00:01, 207.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|███████████████████████████████████████████████████████████████████████████████▌   | 6382/6657 [00:18<00:01, 205.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|███████████████████████████████████████████████████████████████████████████████▊   | 6403/6657 [00:19<00:01, 205.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|████████████████████████████████████████████████████████████████████████████████   | 6425/6657 [00:19<00:01, 209.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|████████████████████████████████████████████████████████████████████████████████▎  | 6446/6657 [00:19<00:01, 209.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|████████████████████████████████████████████████████████████████████████████████▋  | 6467/6657 [00:19<00:00, 205.30it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|████████████████████████████████████████████████████████████████████████████████▉  | 6489/6657 [00:19<00:00, 208.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████████████████████████████████████████████████████████████████████████████▏ | 6510/6657 [00:19<00:00, 207.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████████████████████████████████████████████████████████████████████████████▍ | 6532/6657 [00:19<00:00, 211.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████████████████████████████████████████████████████████████████████████████▋ | 6554/6657 [00:19<00:00, 213.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████████████████████████████████████████████████████████████████████████████▉ | 6576/6657 [00:19<00:00, 209.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|██████████████████████████████████████████████████████████████████████████████████▎| 6597/6657 [00:19<00:00, 207.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|██████████████████████████████████████████████████████████████████████████████████▌| 6618/6657 [00:20<00:00, 202.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 6657/6657 [00:20<00:00, 328.31it/s]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def parse_args():\n",
    "  \"\"\"arguments\"\"\"\n",
    "  config = {\n",
    "    \"data_dir\": \"./Dataset\",\n",
    "    \"model_path\": \"./model.ckpt\",\n",
    "    \"output_path\": \"./output.csv\",\n",
    "  }\n",
    "\n",
    "  return config\n",
    "\n",
    "\n",
    "def main(\n",
    "  data_dir,\n",
    "  model_path,\n",
    "  output_path,\n",
    "):\n",
    "  \"\"\"Main function.\"\"\"\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  print(f\"[Info]: Use {device} now!\")\n",
    "\n",
    "  mapping_path = Path(data_dir) / \"mapping.json\"\n",
    "  mapping = json.load(mapping_path.open())\n",
    "\n",
    "  dataset = InferenceDataset(data_dir)\n",
    "  dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=inference_collate_batch,\n",
    "  )\n",
    "  print(f\"[Info]: Finish loading data!\",flush = True)\n",
    "\n",
    "  speaker_num = len(mapping[\"id2speaker\"])\n",
    "  model = Classifier(n_spks=speaker_num).to(device)\n",
    "  # new code to automatically find and load the best model\n",
    "  list_of_files = glob.glob(f\"{model_dir}/model_*.ckpt\")  # get list of all model files\n",
    "  latest_file = max(list_of_files, key=lambda x: float(x.split('_')[2].split('.')[0]))  # get the best model file\n",
    "  model.load_state_dict(torch.load(latest_file))\n",
    "  model.eval()\n",
    "  print(f\"[Info]: Finish creating model!\",flush = True)\n",
    "\n",
    "  results = [[\"Id\", \"Category\"]]\n",
    "  for feat_paths, mels in tqdm(dataloader):\n",
    "    with torch.no_grad():\n",
    "      mels = mels.to(device)\n",
    "      outs = model(mels)\n",
    "      preds = outs.argmax(1).cpu().numpy()\n",
    "      for feat_path, pred in zip(feat_paths, preds):\n",
    "        results.append([feat_path, mapping[\"id2speaker\"][str(pred)]])\n",
    "  \n",
    "  with open(output_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerows(results)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main(**parse_args())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW04 (1).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
